#### 登陆到跳板机
```
ssh -p 54073 root@103.78.229.68
```

### 不同主机之间文件拷贝


#### 1.  mac开发本的文件拷贝到跳板机
在mac笔记本操作：
旧地址：
```
$ scp -rpP 62534 -i ~/.ssh/id_rsa lotus root@222.189.237.2:/home/ligang/
```

新地址：
```
$ scp -rpP 54073 -i ~/.ssh/id_rsa lotus-server root@103.78.229.68:/home/zhenglun/
```

#### 2. 跳板机拷贝到服务器， 服务器拷贝到跳板机
两个方向的拷贝动作都在跳板机上操作。 

1. 跳板机上的文件拷贝到服务器， 
```
[root@yangzhou010010001015 20200612]# scp -rpP 62534 * 10.10.19.17:/home/fil
```

2. 服务器拷贝跳板机
```
[root@yangzhou010010001015 20200612]# scp -rpP 62534  10.10.19.17:/home/fil *
```


很早之前， 创世节点地址， 被中国强了

p4 worker设置为1 的原因， P4会全核跑， 比如有12个核， 12个核会一起跑， 如果大于1个， 会来回切换， 不如只设置成1个， 如果用docker, 就可以设置每个docker运行多少个核， 所以可以多一些docker, 每个docker跑一个worker. 

同理p23

P1不会全核跑， 所以可以同时起多个P1

怎么整都整不起来

捞到本地

你把你需要的能放上去的， 全都放上去
需要你们现场排查
其实是有办法的， 
没有预留跳板机， 

搞几个虚拟机， 需要什么环境都装上， 有可视化界面， 不用命令行调， 太慢。 
刷的结果都不一致， ‘
那一组没报错， 
这个我可以先整
可能版本

三个版本， 轮流测

interofnet还可以




就近调试


#### 排查链不能同步的问题
lotus chain list 停止了很长时间不动，一是整个链不动了， 一是自己没有链到了高度比较高的节点， 

如果已经链了很多节点， 找到创世节点的链， 

公网上， 不是每个节点都有出块权，算力要大于一个限定值，才有出块权，一般叫做算力出线。 因为出块是在poster里完成的， poster.log里有尝试出块的log， 搜索base height 关键字， 

私有网没个限制， 任何有算力的节点都可以尝试出现。 最后确认这个版本的lotus存在bug, 导致链不能增长。 



##### 

本地考到远程
[root@jumpserver01-yz zhenglun]# scp -rpP 62534 lotus* 10.10.11.39:/home/fil


创世节点地址： 10.10.12.51， 
创世节点是8M， 普通节点可以是512. 

一天300T， 需要多少台机器
展现实力的时候

有两个亿， 翘两百亿的市场。 
买amd的要疯了。

### 测试用例

post 连接到低的高度的链， 会影响出块率， 
每次去查高度, 
99%的精力，解决1%的问题

如果分叉， 更麻烦
lotus给lout-server上报，  加入高度， lotus 找出其中一个，返回给poster

#### 用例： 解决lotus重启问题
##### 1.  vip方式
假如lotus , 坏了， 两台, A和B vip飘到了B
vip票过，但tcp链接有问题， tcp是假链接， 只有重启，所以还要重启

重连后， ip地址可以不变， 包括.lotus不变， 
要么运维重启， 要么程序信号量， 即软重启， 

##### 2. nginx方式
vip pk nginx
nginx 部署多个lotus,  可以重链

health-check, 谁发生了发生， 去三个链的高度， 判读有没有分叉 

准备好两个节点， 

不走nginx, ip就可能变。 tcp链接， 

防火墙贵的原因 ：
就解决session, 同步， 在客户端没有感觉下， 把链接建立到另一个链接。 

vip 两个陪跑。

##### 3. lotus-server方式
lotus-server发信号量， 而故障是被动的 
message 对sealer无感的， 用到的时候重链， 最终目的， 在非常短的时间 重链。 


lotus-server 负载均衡， lotus-server运行在k8s里
nginx , 模拟发送大量的请求， 看lotus-sever成熟的压力怎么样，兵法写数据

kill 掉任何lotus-server

---
延迟高的问题影响。  

---
#### 用例： lotus-message 多个节点操作数据库， 大批量请求， 有没有锁表， 
成功率

--- 
#### 用例： nonce 偏小的问题， 
分叉之后，链上比本地的nonce低， 
缺失的nonce重新广播一遍

转完帐， 再转一次帐 

nonce 

消息的院子性由数据库保证

产生的消息没发到链上， 
---
#### 模拟坏盘的情况

目前能预测的问题， 

一个sealer， 有20个任务， 不断领新的任务， 其他的force-remotework就会先在那里。 

任务的总量的控制， 直接领新的任务， 不段消耗任务的总量。 

每个P的时间， 也要统计。 

force worker任务的成功率， 不能低于

百万fil的时候


---
#### 用例： 模拟force 因网络问题上报不了任务的问题， 网络原因， 上报一次， 

-------
sealer
一个任务很久了， 是不是要告警
领了一个很久了

---

sealer lotus 丢包时， 要重发一次。 

---
sector id损坏的时候，   接下来做windowpost,   
主动上报损坏， 

---
容错的用列

---
10个p1 ， 一个p挂掉了， 整个组都会出问题， 整个组都会停掉， 自己出了问题， 
任务失败了， 任务总量会一直下降， 不一定磁盘导致的
被节点反复领， 状态为2，  


在线编辑


#### deadline
sector 到哪个deadline . 

做的deadline , 是随机的， 

新作的， 下一个周期才知道deadline

那个时间
手动创造， 上报就没了， 上报一个就没了， 自动有这么

windowpost完成时间， 32G 40分钟就够了



### 不同主机之间文件拷贝
```
[root@yangzhou010010001015 20200612]# scp -rpP 62534 * 10.10.19.17:/home/fil
```





对标
边缘覆盖

### 跳板机
跳板机地址，
ssh -p 62534 root@222.189.237.2

从跳板机登陆到分配给自己的主机地址
[fil@yangzhou010010001015 ~]$ ssh -p 62534 10.10.19.17
Last login: Sat Jun 13 17:47:31 2020

### 数据库
#### 连接1
数据库地址：
222.189.237.8
端口
33060
用户
root
数据库密码： 
Ipfs@123ky

#### 连接2
Conn = "root:Ipfs@123ky@tcp(10.10.19.15:3306)/lotus17?loc=Local&parseTime=true"

#### 测试数据库联通：
```
[root@yangzhou010010019017 fil]# telnet 10.10.19.15 3306
Trying 10.10.19.15...
Connected to 10.10.19.15.
Escape character is '^]'.
J
5.7.30
```

### 架构

####  lotus部署
扬州 两个512M的集群 和 一个官方集群 ，  目前GPU只有一个， 运算poster会有压力， poster风险比较大

宁波 32G 集群 

30万 sector 测试， 和1P+ power test

#### 数据检索 

随机抽取一个高度， 得到一个sector,  假如抽取的高度中， 其中有个高度是我们的， 

数据丢失了，  一个sector就32G， 惩罚一次， 算力 就会降下去了，

算力。就降下去了， 如果一次打呗， 

随机抽取三个高度， 惩罚的规则， 

当天就要吧数据就找出来， 在下一个周期， 必须传上去， 恢复上去， 

在做一个验证， 如果验证是被，就永久失效， 

数据存到不通的地方， 

官方的lotus

自己开发的lotus,  

lotus 

数据上链， lotus 完成上链， 
#### sealer 

api  和token给 sealer
32G参数控制， 分发多少个任务， 具体多少个任务要根据做服务器的配置。 
32G的区块， 需要内存70多G，  
目前服务器的配置， 48核，  128 G内存， 
 
最大32G， 
分发
poster用gpu，     
100个机会，要挂载到poster

#### 存储架构
force客户端，   plarm服务端， 和存储服务起对应， 
force.conf

每个存储节点跑plarm 
取代poster挂载

forcefs 

sealer只要知道存储在那 

copytask 

pc 阶段的日志， 

group 


#### ssh 看后端服务

查看userbackend log：

```
ssh root@192.168.1.207
密码： Zaq12wsX
cd /home/forcepool/backend-user/code/logs
[root@shanghai192168001207 logs]# less forcepool-user.log

```


#### 后端服务部署
redis 用aws云服务商， 东京一个， 伦敦备用一个

wallet filescan , 都用一个lotus
mongodb, 数据导出来

nginx 负载均衡， 9600 端口


#### lotus 查看高度和算力等的网站
https://interopnet.filscan.io/#/

https://filfox.io/

https://stats.testnet.filecoin.io/d/3j4oi32ore0rfjos/chain-internal?orgId=1&refresh=45s

开发组， 项目组

基础服务， 
fellcoin .  

force sport 
wallet 

### 运维监控
aws forcepool 服务器监控
http://monitor.filscan.cn:3000/login


原力区作业平台
http://paas.ipfsyuanli.com/login/?app_id=bk_job&c_url=http%3A%2F%2Fjob.ipfsyuanli.com%2F


###  开发常用网站

任务进展图：
https://www.leangoo.com/kanban/board_list#/home/list#myBoard
ligang@ipfsforce.com
fsyxjsxw1

forcepool外网：
https://forcepool.io/account?key=register
18521721867
Fsjsjsjx1

jekins编译和版本发布:
http://jenkins.forceup.in/
用户名：ligang
密码：ipfsforce

confluence 项目任务清单 和 开发文档分享：
http://192.168.1.172:8090/welcome.action
ligang
lyqx

新浪邮箱：
kylinkylin99abc
fsjsjsjx1

阿里云 账号：   spacefeasible


### 代码提交
```
git push origin feature/codeservice

$ git fetch origin
LdeMacBook-Pro:UserBackend zhenglun1$ git merge origin/test
Updating 4ee2147..1fc6361
error: Your local changes to the following files would be overwritten by merge:
	go.mokd
Please commit your changes or stash them before you merge.
Aborting

LdeMacBook-Pro:UserBackend zhenglun1$ git checkout -f .
Updated 1 path from the index

LdeMacBook-Pro:UserBackend zhenglun1$ git merge origin/test
Updating 4ee2147..1fc6361
Fast-forwardco
 api/types/request.go            |   4 +

git push origin feature/codeservice
```
